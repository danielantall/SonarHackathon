{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "na2Q_T-nwoJf",
    "outputId": "56479fb6-2cda-411b-97e4-1a82130a490f"
   },
   "outputs": [],
   "source": [
    "pip install --upgrade datasets huggingface_hub fsspec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eVtJBXXrmWv9",
    "outputId": "4d2e9607-714e-489b-fdd5-c4f9ff530fe0"
   },
   "outputs": [],
   "source": [
    "# gpu if necessary by hf\n",
    "!pip install accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B4u055ykGw1G"
   },
   "source": [
    "## PREPROCESSING & CLEANING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jLvRolo-8kqP"
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "# dataset manipulation and math\n",
    "import polars as pl\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "\n",
    "# ml\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# visuals\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# misc\n",
    "import os\n",
    "import json\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "61GV4d6y7vqk"
   },
   "outputs": [],
   "source": [
    "# Setup Reusable Code for Logging\n",
    "\n",
    "# to see the cleaning visuals for analysis\n",
    "os.makedirs(\"cleaning_visualizations\", exist_ok=True)\n",
    "\n",
    "# Set up logging format\n",
    "def log_section(title):\n",
    "    print(f\" {title} \".center(80, \"=\"))\n",
    "\n",
    "# each step in the log (ex. Loading dataset)\n",
    "def log_step(step_name):\n",
    "    print(f\"\\n--- {step_name} ---\\n\")\n",
    "\n",
    "# log each metric as needed\n",
    "def log_metric(name, value):\n",
    "    print(f\"  • {name}: {value}\")\n",
    "\n",
    "# compare before and after step to analyze difference\n",
    "def log_comparison(name, before, after, unit=\"rows\"):\n",
    "    diff = after - before if isinstance(before, (int, float)) else 0\n",
    "    diff_percent = (diff / before * 100) if before > 0 else 0\n",
    "    if diff < 0:\n",
    "        print(f\"  • {name}: {before:,} → {after:,} {unit} ({abs(diff):,} removed, {abs(diff_percent):.2f}% reduction)\")\n",
    "    else:\n",
    "        print(f\"  • {name}: {before:,} → {after:,} {unit} ({diff:,} added, {diff_percent:.2f}% increase)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 371,
     "referenced_widgets": [
      "5dae43ecc1394e9ba023afae42fd8190",
      "a5d82022994b487b9fa9ccf0e3844b32",
      "5a12f05a2daa4a65949f47e24ea10e71",
      "994f1213c1dc4b5b8a72267e57b1c3cf",
      "2ed4d15ffc3142ccb847ca8be81dcb06",
      "696d4c92d8074b849a597fe3cd436319",
      "775dbfd4801a44e79cd2ccfcda874be5",
      "cfca9872fe2f4fe1892e20963dd9530f",
      "5b73bac6eede47148d699520df2f678e",
      "66c3c64827564b5fb06cbf577b8a6f2b",
      "d5db6dabf44d454cbf04fd9cbbdb31f5",
      "532f87c93b424906b1b3296cae906436",
      "b9d59e7dd7b449009b7e39827e9592fc",
      "c32433735d70415e951736aa32469155",
      "deda463ea44542aa9c4901f2cdd3002d",
      "fab5c2d51dfb43a0a01e893eb922974b",
      "daae9ec2344448b181ce8dc035ec24b8",
      "9e263f58005e46d2b2c5062025576bb5",
      "5e100fe14fa945de9df8b6a1fa3914ef",
      "c15eb965b8de4ccaa6b489707ddf373a",
      "bc5da9d6ee664d4f8f813db27aadcae1",
      "bdfc888c83b34bc5851d9761677bd1f0",
      "e895c119cc69400e8f7dc24fb4e3dee4",
      "259a677b288b4f01b72289b0ecb1b055",
      "7cddc057018244acb8628c18e5f4bf63",
      "acffaf4f03f145f4a76b8e4923186827",
      "bd8f112c4c6b4f77aa37668c5805c2f4",
      "4c4ac89e6ff84a299c31c2ff1cf25c4d",
      "382996bababb4d39ba403fc32a934284",
      "a15ca61a5cda447caa246af5ae285b49",
      "8feccb4bf0c441be9101491683c91665",
      "5f728d8b43e34c799ebdb95a3328e31d",
      "4dc1323dcadb487eaa056f7dfa4715ef",
      "98c84477e3a043238da537111f92a912",
      "e64c9a6785494953883f243ef6ea338e",
      "7fe3ecc7de174768bae75febce21a66d",
      "91f726fcd40246ccba10653077a608ea",
      "be7337925de34e96b777569a58662b7a",
      "a11aaf456c414a12ae1ab6099f14cc43",
      "2a493c3089144f17beac1e0ce9c12ab7",
      "61d54a905ef54eddabff4fe33d00d778",
      "60df1fc4d4374ce2b522f44d8c42e013",
      "dd780e9006a640b4868eabc8c7f97660",
      "6151b6b862be4ac58046e5183a260f66"
     ]
    },
    "id": "SuHa5fvY8QVu",
    "outputId": "211a477b-8ed6-42ac-c65d-f8f90d50e4d0"
   },
   "outputs": [],
   "source": [
    "# Load dataset and analyze nulls\n",
    "\n",
    "# Load Dataset\n",
    "log_section(\"Dataset Loading\")\n",
    "log_step(\"Load the Mental Health Dataset from HuggingFace.\")\n",
    "\n",
    "ds = load_dataset(\"ShenLab/MentalChat16K\")\n",
    "# copy the data\n",
    "all_data = pl.DataFrame(ds[\"train\"][:])\n",
    "\n",
    "log_metric(\"Total examples\", all_data.shape[0])\n",
    "log_metric(\"Columns\", all_data.columns)\n",
    "log_metric(\"Data types\", all_data.dtypes)\n",
    "\n",
    "# Check for null values in original dataset\n",
    "null_counts = {\n",
    "    \"instruction\": all_data.filter(pl.col(\"instruction\").is_null()).shape[0],\n",
    "    \"input\": all_data.filter(pl.col(\"input\").is_null()).shape[0],\n",
    "    \"output\": all_data.filter(pl.col(\"output\").is_null()).shape[0]\n",
    "}\n",
    "\n",
    "# analyze the nulls\n",
    "log_step(\"Null value analysis\")\n",
    "for col, count in null_counts.items():\n",
    "    log_metric(f\"Null values in '{col}'\", f\"{count:,} ({count/all_data.shape[0]*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UTkv36kl81Kc",
    "outputId": "81fe9de5-a099-4cb9-ea82-8e5f166637dd"
   },
   "outputs": [],
   "source": [
    "# Train-Test Split\n",
    "\n",
    "# Split Dataset\n",
    "log_section(\"Train/Test/Val Split\")\n",
    "log_step(\"Creating data splits in the dataset.\")\n",
    "\n",
    "# 80% train, 20% test\n",
    "train_df, temp_df = train_test_split(all_data, test_size=0.2, random_state=42)\n",
    "#subsplit within test; now 10% test and 10% val (for hyperparameter tuning)\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
    "\n",
    "log_metric(\"Training set size\", f\"{train_df.shape[0]:,} ({train_df.shape[0]/all_data.shape[0]*100:.1f}%)\")\n",
    "log_metric(\"Validation set size\", f\"{val_df.shape[0]:,} ({val_df.shape[0]/all_data.shape[0]*100:.1f}%)\")\n",
    "log_metric(\"Test set size\", f\"{test_df.shape[0]:,} ({test_df.shape[0]/all_data.shape[0]*100:.1f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "mnZsiZ9T9VZX",
    "outputId": "c21b0049-86be-4dd2-f7a4-5e16f04b6fbc"
   },
   "outputs": [],
   "source": [
    "# Calculate Length Statistics and Visualize\n",
    "\n",
    "log_section(\"Analysis Of Text Length\")\n",
    "log_step(\"Calculating text length statistics to treat potential outliers.\")\n",
    "\n",
    "# Add length columns to training data\n",
    "train_with_lens = train_df.with_columns([\n",
    "    pl.col(\"instruction\").str.len_chars().alias(\"instruction_len\"),\n",
    "    pl.col(\"input\").str.len_chars().alias(\"input_len\"),\n",
    "    pl.col(\"output\").str.len_chars().alias(\"output_len\")\n",
    "])\n",
    "\n",
    "# Calculate statistics for each column (intorduction, input, output)\n",
    "stats = {\n",
    "    \"instruction\": {\n",
    "        \"min\": train_with_lens[\"instruction_len\"].min(),\n",
    "        \"max\": train_with_lens[\"instruction_len\"].max(),\n",
    "        \"mean\": train_with_lens[\"instruction_len\"].mean(),\n",
    "        \"median\": train_with_lens[\"instruction_len\"].median(),\n",
    "        \"p05\": train_with_lens.select(pl.col(\"instruction_len\").quantile(0.05)).item(),\n",
    "        \"p95\": train_with_lens.select(pl.col(\"instruction_len\").quantile(0.95)).item()\n",
    "    },\n",
    "    \"input\": {\n",
    "        \"min\": train_with_lens[\"input_len\"].min(),\n",
    "        \"max\": train_with_lens[\"input_len\"].max(),\n",
    "        \"mean\": train_with_lens[\"input_len\"].mean(),\n",
    "        \"median\": train_with_lens[\"input_len\"].median(),\n",
    "        \"p05\": train_with_lens.select(pl.col(\"input_len\").quantile(0.05)).item(),\n",
    "        \"p95\": train_with_lens.select(pl.col(\"input_len\").quantile(0.95)).item()\n",
    "    },\n",
    "    \"output\": {\n",
    "        \"min\": train_with_lens[\"output_len\"].min(),\n",
    "        \"max\": train_with_lens[\"output_len\"].max(),\n",
    "        \"mean\": train_with_lens[\"output_len\"].mean(),\n",
    "        \"median\": train_with_lens[\"output_len\"].median(),\n",
    "        \"p05\": train_with_lens.select(pl.col(\"output_len\").quantile(0.05)).item(),\n",
    "        \"p95\": train_with_lens.select(pl.col(\"output_len\").quantile(0.95)).item()\n",
    "    }\n",
    "}\n",
    "\n",
    "# log stats for the length statistics\n",
    "for field, field_stats in stats.items():\n",
    "    log_step(f\"{field.capitalize()} length statistics\")\n",
    "    for stat_name, value in field_stats.items():\n",
    "        log_metric(stat_name, f\"{value:.1f}\" if isinstance(value, float) else f\"{value:,}\")\n",
    "\n",
    "log_step(\"Generating length distribution visualizations\")\n",
    "\n",
    "# Visualize with histogram for further analysis (default values for consistency)\n",
    "plt.rcParams.update({\n",
    "    \"axes.titlesize\": 18,\n",
    "    \"axes.labelsize\": 14,\n",
    "    \"xtick.labelsize\": 12,\n",
    "    \"ytick.labelsize\": 12,\n",
    "    \"legend.fontsize\": 12\n",
    "})\n",
    "\n",
    "# Visualize each field in its own figure\n",
    "for field in [\"instruction\", \"input\", \"output\"]:\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.histplot(train_with_lens[f\"{field}_len\"], kde=True)\n",
    "    plt.axvline(x=stats[field][\"p05\"], color='r', linestyle='--', label='5th percentile')\n",
    "    plt.axvline(x=stats[field][\"p95\"], color='g', linestyle='--', label='95th percentile')\n",
    "    plt.title(f\"{field.capitalize()} Length Distribution\", fontsize=20)\n",
    "    plt.xlabel(\"Character Length\", fontsize=14)\n",
    "    plt.ylabel(\"Frequency\", fontsize=14)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    file_path = f\"cleaning_visualizations/{field}_length_distribution.png\"\n",
    "    plt.savefig(file_path)\n",
    "    log_metric(f\"{field.capitalize()} length distribution saved\", file_path)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bfEaUeIz-a-k",
    "outputId": "4f8b0473-fd8c-47c6-f729-dfece4b93784"
   },
   "outputs": [],
   "source": [
    "# Set Data Thresholds\n",
    "log_section(\"Data Thresholds\")\n",
    "log_step(\"Setting data thresholds based on percentiles for outlier treatment\")\n",
    "\n",
    "# 5th percentile is minimum threshold\n",
    "input_min_len = stats[\"input\"][\"p05\"]\n",
    "output_min_len = stats[\"output\"][\"p05\"]\n",
    "\n",
    "log_metric(\"Input minimum length threshold (5th percentile)\", f\"{input_min_len:.1f} characters\")\n",
    "log_metric(\"Output minimum length threshold (5th percentile)\", f\"{output_min_len:.1f} characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yRyZjEDN_C9G",
    "outputId": "cc33c634-32bf-43d7-c38b-ae83e0ec8e40"
   },
   "outputs": [],
   "source": [
    "# Define function to clean the data\n",
    "log_section(\"Cleaning the dataset.\")\n",
    "\n",
    "def clean_dataset(df, thresholds, split_name):\n",
    "    log_step(f\"Cleaning {split_name} split\")\n",
    "\n",
    "    original_size = df.shape[0]\n",
    "    log_metric(\"Original size\", f\"{original_size:,} rows\")\n",
    "\n",
    "    # 1) Remove nulls\n",
    "    step1_df = df.filter(~pl.col(\"input\").is_null())\n",
    "    nulls_removed = original_size - step1_df.shape[0]\n",
    "    log_metric(\"Null values removed\", f\"{nulls_removed:,} ({nulls_removed/original_size*100:.2f}% of original)\")\n",
    "\n",
    "    # 2) Trim whitespace\n",
    "    step2_df = step1_df.select([\n",
    "        pl.col(\"instruction\").str.strip_chars().alias(\"instruction\"),\n",
    "        pl.col(\"input\").str.strip_chars().alias(\"input\"),\n",
    "        pl.col(\"output\").str.strip_chars().alias(\"output\")\n",
    "    ])\n",
    "\n",
    "    # 3) Remove duplicates\n",
    "    step3_df = step2_df.unique()\n",
    "    duplicates_removed = step2_df.shape[0] - step3_df.shape[0]\n",
    "    log_metric(\"Duplicates removed\", f\"{duplicates_removed:,} ({duplicates_removed/step2_df.shape[0]*100:.2f}% of whitespace-trimmed)\")\n",
    "\n",
    "    # 4) Filter by length\n",
    "    step4_df = step3_df.filter(\n",
    "        (pl.col(\"input\").str.len_chars() >= thresholds[\"input_min\"]) &\n",
    "        (pl.col(\"output\").str.len_chars() >= thresholds[\"output_min\"])\n",
    "    )\n",
    "    length_filtered = step3_df.shape[0] - step4_df.shape[0]\n",
    "    log_metric(\"Short examples filtered\", f\"{length_filtered:,} ({length_filtered/step3_df.shape[0]*100:.2f}% of deduplicated)\")\n",
    "\n",
    "    # Final summary\n",
    "    total_removed = original_size - step4_df.shape[0]\n",
    "    log_metric(\"Total examples removed\", f\"{total_removed:,} ({total_removed/original_size*100:.2f}% of original)\")\n",
    "    log_metric(\"Final size\", f\"{step4_df.shape[0]:,} rows\")\n",
    "\n",
    "    return step4_df\n",
    "\n",
    "# Apply cleaning to all splits with same thresholds to maintain consistency\n",
    "thresholds = {\"input_min\": input_min_len, \"output_min\": output_min_len}\n",
    "train_clean = clean_dataset(train_df, thresholds, \"training\")\n",
    "val_clean = clean_dataset(val_df, thresholds, \"validation\")\n",
    "test_clean = clean_dataset(test_df, thresholds, \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Le_1SbJTGC3u",
    "outputId": "d095801a-ffad-434e-8cce-dec4fffd3f5e"
   },
   "outputs": [],
   "source": [
    "log_section(\"After Cleaning Analysis\")\n",
    "\n",
    "# Recalculate length columns on cleaned training set\n",
    "train_clean_with_lens = train_clean.with_columns([\n",
    "    pl.col(\"instruction\").str.len_chars().alias(\"instruction_len\"),\n",
    "    pl.col(\"input\").str.len_chars().alias(\"input_len\"),\n",
    "    pl.col(\"output\").str.len_chars().alias(\"output_len\")\n",
    "])\n",
    "\n",
    "# Optional: recalculate stats if needed for updated percentiles\n",
    "# Or reuse existing ones if you want consistent reference lines\n",
    "log_step(\"Generating length distribution visualizations after cleaning.\")\n",
    "log_step(\"Inputs and outputs below the 5th percentile were removed due to being too short to be meaningful—especially since our inputs simulate user speech.\")\n",
    "log_step(\"We retained examples above the 95th percentile, as longer inputs and responses may provide richer context.These can be revisited later if a token limit is required.\")\n",
    "\n",
    "print(\"\\n)\")\n",
    "\n",
    "# Reuse same style as the BEFORE graphs\n",
    "plt.rcParams.update({\n",
    "    \"axes.titlesize\": 18,\n",
    "    \"axes.labelsize\": 14,\n",
    "    \"xtick.labelsize\": 12,\n",
    "    \"ytick.labelsize\": 12,\n",
    "    \"legend.fontsize\": 12\n",
    "})\n",
    "\n",
    "# Generate and save new plots\n",
    "for field in [\"instruction\", \"input\", \"output\"]:\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.histplot(train_clean_with_lens[f\"{field}_len\"], kde=True)\n",
    "\n",
    "    # (Optional) Add original percentiles from uncleaned data\n",
    "    plt.axvline(x=stats[field][\"p05\"], color='r', linestyle='--', label='Original 5th percentile')\n",
    "    plt.axvline(x=stats[field][\"p95\"], color='g', linestyle='--', label='Original 95th percentile')\n",
    "\n",
    "    plt.title(f\"{field.capitalize()} Length Distribution (After Cleaning)\", fontsize=20)\n",
    "    plt.xlabel(\"Character Length\", fontsize=14)\n",
    "    plt.ylabel(\"Frequency\", fontsize=14)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "\n",
    "    file_path = f\"cleaning_visualizations/{field}_length_distribution_after_cleaning.png\"\n",
    "    plt.savefig(file_path)\n",
    "    log_metric(f\"{field.capitalize()} cleaned length distribution saved\", file_path)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OR5l-G98_H_4",
    "outputId": "4234896c-caa5-45af-9f03-11d6533173a4"
   },
   "outputs": [],
   "source": [
    "# Final Dataset Summary\n",
    "\n",
    "log_section(\"Final Summary\")\n",
    "log_step(\"Dataset sizes after cleaning\")\n",
    "log_comparison(\"Training set\", train_df.shape[0], train_clean.shape[0])\n",
    "log_comparison(\"Validation set\", val_df.shape[0], val_clean.shape[0])\n",
    "log_comparison(\"Test set\", test_df.shape[0], test_clean.shape[0])\n",
    "log_comparison(\"Total dataset\", all_data.shape[0], train_clean.shape[0] + val_clean.shape[0] + test_clean.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xYqW9ULJ_N5r",
    "outputId": "9f5f964e-851a-40e6-c2eb-869a556f8938"
   },
   "outputs": [],
   "source": [
    "# Save Cleaned Datasets\n",
    "\n",
    "log_section(\"Saving Cleaned Datasets\")\n",
    "log_step(\"Saving the cleaned datasets for future use.\")\n",
    "# Convert back to Hugging Face dataset format if needed\n",
    "# Or save as Parquet/CSV files\n",
    "train_clean.write_parquet(\"cleaned_train.parquet\")\n",
    "val_clean.write_parquet(\"cleaned_val.parquet\")\n",
    "# Convert back to Hugging Face dataset format if needed\n",
    "# save as parquet for now\n",
    "train_clean.write_parquet(\"cleaned_train.parquet\")\n",
    "val_clean.write_parquet(\"cleaned_val.parquet\")\n",
    "test_clean.write_parquet(\"cleaned_test.parquet\")\n",
    "\n",
    "log_metric(\"Training set saved\", \"cleaned_train.parquet\")\n",
    "log_metric(\"Validation set saved\", \"cleaned_val.parquet\")\n",
    "log_metric(\"Test set saved\", \"cleaned_test.parquet\")\n",
    "print(\"\\n\")\n",
    "log_section(\"Cleaning complete.\")\n",
    "log_step(\"The data has been cleaned, split, and images have been saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "akXaymhoGbq8"
   },
   "source": [
    "## TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AcIvFlxrNA7b",
    "outputId": "01d96f27-b4e4-4de3-f1c2-08bf0dd4ea2c"
   },
   "outputs": [],
   "source": [
    "log_section(\"Adding Metadata (Emotion Tags)\")\n",
    "log_step(\"Associating emotion tags with dataset entries.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KzdHRwhHNr0F"
   },
   "outputs": [],
   "source": [
    "def add_metadata_to_datasets(train_clean, val_clean, test_clean):\n",
    "    \"\"\"\n",
    "    Add emotion and severity metadata to the cleaned datasets.\n",
    "    \"\"\"\n",
    "    log_section(\"ADDING METADATA\")\n",
    "    log_step(\"Adding emotion and severity metadata to datasets\")\n",
    "\n",
    "    # Define emotion categories and severity levels\n",
    "    emotions = [\"anxiety\", \"sadness\", \"frustration\", \"hope\", \"neutral\", \"fear\", \"gratitude\"]\n",
    "    severity_levels = [\"low\", \"medium\", \"high\"]\n",
    "\n",
    "    # Simple rule-based emotion and severity detection\n",
    "    def detect_emotion_and_severity(text):\n",
    "        if text is None:\n",
    "            return \"neutral\", \"low\"\n",
    "\n",
    "        text = text.lower()\n",
    "\n",
    "        # Simple keyword matching for emotions\n",
    "        emotion_keywords = {\n",
    "            \"anxiety\": [\"anxious\", \"worry\", \"nervous\", \"panic\", \"stress\", \"tense\", \"uneasy\"],\n",
    "            \"sadness\": [\"sad\", \"depressed\", \"unhappy\", \"miserable\", \"down\", \"blue\", \"grief\"],\n",
    "            \"frustration\": [\"frustrated\", \"annoyed\", \"irritated\", \"angry\", \"upset\", \"mad\"],\n",
    "            \"hope\": [\"hope\", \"optimistic\", \"better\", \"improve\", \"progress\", \"positive\"],\n",
    "            \"fear\": [\"afraid\", \"scared\", \"terrified\", \"fear\", \"dread\", \"frightened\"],\n",
    "            \"gratitude\": [\"thank\", \"grateful\", \"appreciate\", \"thankful\", \"blessed\"],\n",
    "            \"neutral\": []  # Default\n",
    "        }\n",
    "\n",
    "        # Count keyword matches for each emotion\n",
    "        emotion_scores = {emotion: 0 for emotion in emotions}\n",
    "        for emotion, keywords in emotion_keywords.items():\n",
    "            for keyword in keywords:\n",
    "                if keyword in text:\n",
    "                    emotion_scores[emotion] += 1\n",
    "\n",
    "        # Determine dominant emotion\n",
    "        dominant_emotion = max(emotion_scores.items(), key=lambda x: x[1])[0]\n",
    "        if emotion_scores[dominant_emotion] == 0:\n",
    "            dominant_emotion = \"neutral\"\n",
    "\n",
    "        # Simple heuristic for severity\n",
    "        severity_indicators = {\n",
    "            \"high\": [\"very\", \"extremely\", \"severe\", \"terrible\", \"worst\", \"unbearable\", \"always\", \"constantly\"],\n",
    "            \"medium\": [\"quite\", \"moderately\", \"somewhat\", \"often\", \"frequently\", \"regularly\"],\n",
    "            \"low\": [\"slightly\", \"mild\", \"occasionally\", \"sometimes\", \"a bit\", \"a little\", \"rarely\"]\n",
    "        }\n",
    "\n",
    "        # Count severity indicators\n",
    "        severity_scores = {level: 0 for level in severity_levels}\n",
    "        for level, indicators in severity_indicators.items():\n",
    "            for indicator in indicators:\n",
    "                if indicator in text:\n",
    "                    severity_scores[level] += 1\n",
    "\n",
    "        # Determine severity level\n",
    "        if severity_scores[\"high\"] > 0:\n",
    "            severity = \"high\"\n",
    "        elif severity_scores[\"medium\"] > 0:\n",
    "            severity = \"medium\"\n",
    "        else:\n",
    "            severity = \"low\"\n",
    "\n",
    "        return dominant_emotion, severity\n",
    "\n",
    "    # Function to add metadata to a dataset\n",
    "    def add_metadata_to_dataset(df, name):\n",
    "        log_step(f\"Adding metadata to {name} dataset\")\n",
    "\n",
    "        # Convert to pandas for easier manipulation\n",
    "        pdf = df.to_pandas()\n",
    "\n",
    "        # Add emotion and severity columns\n",
    "        emotions = []\n",
    "        severities = []\n",
    "\n",
    "        for _, row in pdf.iterrows():\n",
    "            emotion, severity = detect_emotion_and_severity(row[\"input\"])\n",
    "            emotions.append(emotion)\n",
    "            severities.append(severity)\n",
    "\n",
    "        # Add as new columns\n",
    "        pdf[\"emotion\"] = emotions\n",
    "        pdf[\"severity\"] = severities\n",
    "\n",
    "        # Convert back to polars\n",
    "        result_df = pl.from_pandas(pdf)\n",
    "\n",
    "        # Log statistics\n",
    "        emotion_counts = pdf[\"emotion\"].value_counts()\n",
    "        severity_counts = pdf[\"severity\"].value_counts()\n",
    "\n",
    "        log_metric(f\"{name} dataset emotion distribution\", emotion_counts.to_dict())\n",
    "        log_metric(f\"{name} dataset severity distribution\", severity_counts.to_dict())\n",
    "\n",
    "        return result_df\n",
    "\n",
    "    # Add metadata to each dataset\n",
    "    train_with_metadata = add_metadata_to_dataset(train_clean, \"Training\")\n",
    "    val_with_metadata = add_metadata_to_dataset(val_clean, \"Validation\")\n",
    "    test_with_metadata = add_metadata_to_dataset(test_clean, \"Test\")\n",
    "\n",
    "    # Visualize metadata distribution\n",
    "    log_step(\"Generating metadata distribution visualizations\")\n",
    "\n",
    "    # Convert to pandas for visualization\n",
    "    train_pdf = train_with_metadata.to_pandas()\n",
    "\n",
    "    # Emotion distribution\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.countplot(data=train_pdf, x=\"emotion\")\n",
    "    plt.title(\"Emotion Distribution in Training Data\")\n",
    "    plt.xlabel(\"Emotion\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"cleaning_visualizations/emotion_distribution.png\")\n",
    "    log_metric(\"Visualization saved\", \"cleaning_visualizations/emotion_distribution.png\")\n",
    "\n",
    "    # Severity distribution\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.countplot(data=train_pdf, x=\"severity\")\n",
    "    plt.title(\"Severity Distribution in Training Data\")\n",
    "    plt.xlabel(\"Severity Level\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"cleaning_visualizations/severity_distribution.png\")\n",
    "    log_metric(\"Visualization saved\", \"cleaning_visualizations/severity_distribution.png\")\n",
    "\n",
    "    # Save datasets with metadata\n",
    "    train_with_metadata.write_parquet(\"train_with_metadata.parquet\")\n",
    "    val_with_metadata.write_parquet(\"val_with_metadata.parquet\")\n",
    "    test_with_metadata.write_parquet(\"test_with_metadata.parquet\")\n",
    "\n",
    "    log_metric(\"Datasets with metadata saved\", \"train_with_metadata.parquet, val_with_metadata.parquet, test_with_metadata.parquet\")\n",
    "\n",
    "    return train_with_metadata, val_with_metadata, test_with_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FofTYDGakZPE"
   },
   "outputs": [],
   "source": [
    "##### NOT DONE OR TESTED CODE BELOW #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xPwLz3rhk78K"
   },
   "outputs": [],
   "source": [
    "#### MIGHT CHANGE FOR RAG #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ad-EUIkkVds4"
   },
   "outputs": [],
   "source": [
    "def generate_enhanced_prompts(train_with_metadata):\n",
    "    log_section(\"ENHANCED PROMPT ENGINEERING\")\n",
    "    log_step(\"Generating enhanced prompts based on prompt engineering guidelines\")\n",
    "    os.makedirs(\"sonar_prompts\", exist_ok=True)\n",
    "\n",
    "\n",
    "    # Base system message that establishes role, communication style, and boundaries\n",
    "    base_system_message = \"\"\"You are a supportive and empathetic AI companion designed to listen to users, offer helpful reflections, and encourage positive coping mechanisms for voice-related concerns. You are not a medical professional and cannot provide diagnoses or treatment advice.\n",
    "\n",
    "Respond in an encouraging, understanding, and non-judgmental tone. Use active listening techniques such as paraphrasing and summarizing to show you're engaged. Keep responses concise and focused on the user's immediate concerns.\n",
    "\n",
    "Guidelines for your responses:\n",
    "- Use emotionally sensitive language that acknowledges the user's feelings\n",
    "- Maintain cultural neutrality in your suggestions and reflections\n",
    "- Ask open-ended questions to encourage user reflection\n",
    "- Focus on active listening rather than immediate solutions\n",
    "- Avoid clinical terminology that might create unrealistic expectations\n",
    "- Encourage users to seek professional help when appropriate\"\"\"\n",
    "\n",
    "    # Define emotion-specific system message additions\n",
    "    emotion_prompts = {\n",
    "        \"anxiety\": \"The user appears to be experiencing anxiety. Acknowledge their feelings of worry or nervousness. Offer simple breathing techniques that can help relax vocal tension. Use a calm, reassuring tone and avoid overwhelming them with too many suggestions at once.\",\n",
    "\n",
    "        \"sadness\": \"The user appears to be experiencing sadness. Validate their feelings without trying to immediately cheer them up. Suggest gentle vocal exercises that can help express emotions. Use a warm, supportive tone and acknowledge that voice work during emotional periods can be challenging.\",\n",
    "\n",
    "        \"frustration\": \"The user appears to be experiencing frustration. Acknowledge their feelings without dismissing them. Suggest focused, structured voice techniques that provide a sense of control. Use a clear, direct tone while maintaining empathy.\",\n",
    "\n",
    "        \"hope\": \"The user appears to be feeling hopeful. Build on this positive emotion by reinforcing their optimism. Suggest progressive voice exercises that can help them track improvement. Use an encouraging tone that matches their positive outlook.\",\n",
    "\n",
    "        \"fear\": \"The user appears to be experiencing fear. Provide reassurance and grounding techniques. Suggest simple voice exercises that can be done in the moment to help manage anxiety. Use a calm, steady tone and avoid overwhelming them with complex information.\",\n",
    "\n",
    "        \"gratitude\": \"The user appears to be expressing gratitude. Acknowledge their positive sentiment and use it as an opportunity to reinforce their progress. Suggest ways to build on their current success. Use a warm, appreciative tone that matches their sentiment.\",\n",
    "\n",
    "        \"neutral\": \"The user's emotional state appears neutral. Provide balanced, informative responses. Suggest evidence-based voice therapy techniques tailored to their specific concerns. Use a professional, supportive tone.\"\n",
    "    }\n",
    "\n",
    "    # Define severity-specific modifications\n",
    "    severity_modifications = {\n",
    "        \"low\": \"The user's concerns appear to be mild. Focus on preventative techniques and maintenance exercises. Keep explanations brief and practical, emphasizing daily habits that can maintain vocal health.\",\n",
    "\n",
    "        \"medium\": \"The user's concerns appear to be moderate. Provide a balance of immediate relief techniques and longer-term exercises. Include moderate detail in explanations, focusing on consistent practice and gradual improvement.\",\n",
    "\n",
    "        \"high\": \"The user's concerns appear to be severe. Prioritize immediate relief strategies first, followed by structured intervention plans. Provide detailed, step-by-step guidance with frequent check-ins. Strongly encourage professional consultation while being supportive.\"\n",
    "    }\n",
    "\n",
    "\n",
    "    # Template for including conversation history\n",
    "    conversation_history_template = \"\"\"\n",
    "Previous conversation:\n",
    "{conversation_history}\n",
    "\n",
    "Current user message:\n",
    "{current_message}\"\"\"\n",
    "\n",
    "    # Template for including summarized conversation\n",
    "    conversation_summary_template = \"\"\"\n",
    "Conversation summary:\n",
    "{conversation_summary}\n",
    "\n",
    "Recent exchanges:\n",
    "{recent_exchanges}\n",
    "\n",
    "Current user message:\n",
    "{current_message}\"\"\"\n",
    "\n",
    "    # Template for including relevant past interactions\n",
    "    relevant_memory_template = \"\"\"\n",
    "Relevant past information:\n",
    "{relevant_memories}\n",
    "\n",
    "Current user message:\n",
    "{current_message}\"\"\"\n",
    "\n",
    "    # Generate combined prompts for each emotion-severity combination\n",
    "    combined_prompts = {}\n",
    "\n",
    "    for emotion, emotion_prompt in emotion_prompts.items():\n",
    "        for severity, severity_prompt in severity_modifications.items():\n",
    "            key = f\"{emotion}_{severity}\"\n",
    "\n",
    "            # Combine base system message with emotion and severity specific guidance\n",
    "            combined_prompts[key] = {\n",
    "                \"system_message\": f\"{base_system_message}\\n\\n{emotion_prompt}\\n\\n{severity_prompt}\",\n",
    "                \"conversation_history_template\": conversation_history_template,\n",
    "                \"conversation_summary_template\": conversation_summary_template,\n",
    "                \"relevant_memory_template\": relevant_memory_template\n",
    "            }\n",
    "\n",
    "    # Save prompts to file\n",
    "    with open(\"sonar_prompts/enhanced_prompts.json\", \"w\") as f:\n",
    "        json.dump(combined_prompts, f, indent=2)\n",
    "\n",
    "    log_metric(\"Enhanced prompts generated\", len(combined_prompts))\n",
    "    log_metric(\"Prompts saved to\", \"sonar_prompts/enhanced_prompts.json\")\n",
    "\n",
    "    # Generate example few-shot learning prompts\n",
    "    log_step(\"Generating few-shot learning examples\")\n",
    "\n",
    "    # Convert to pandas for easier filtering\n",
    "    train_pdf = train_with_metadata.to_pandas()\n",
    "\n",
    "    few_shot_examples = {}\n",
    "\n",
    "    # For each emotion-severity combination, find a good example\n",
    "    for emotion in emotion_prompts.keys():\n",
    "        for severity in severity_modifications.keys():\n",
    "            key = f\"{emotion}_{severity}\"\n",
    "\n",
    "            # Filter dataset\n",
    "            filtered = train_pdf[(train_pdf[\"emotion\"] == emotion) & (train_pdf[\"severity\"] == severity)]\n",
    "\n",
    "            if len(filtered) > 0:\n",
    "                # Take the first example (or you could implement better selection logic)\n",
    "                example = filtered.iloc[0]\n",
    "\n",
    "                few_shot_examples[key] = {\n",
    "                    \"input\": example[\"input\"],\n",
    "                    \"output\": example[\"output\"]\n",
    "                }\n",
    "            else:\n",
    "                few_shot_examples[key] = {\n",
    "                    \"input\": \"No example available for this combination\",\n",
    "                    \"output\": \"No example available for this combination\"\n",
    "                }\n",
    "\n",
    "    # Save few-shot examples to file\n",
    "    with open(\"sonar_prompts/few_shot_examples.json\", \"w\") as f:\n",
    "        json.dump(few_shot_examples, f, indent=2)\n",
    "\n",
    "    log_metric(\"Few-shot examples generated\", len(few_shot_examples))\n",
    "    log_metric(\"Examples saved to\", \"sonar_prompts/few_shot_examples.json\")\n",
    "\n",
    "    return {\n",
    "        \"system_prompts\": combined_prompts,\n",
    "        \"few_shot_examples\": few_shot_examples\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9L9OBtqlkAbY"
   },
   "outputs": [],
   "source": [
    "#setup sonar api\n",
    "\n",
    "def setup_sonar_api():\n",
    "    \"\"\"\n",
    "    Set up the Sonar API client.\n",
    "    \"\"\"\n",
    "    log_section(\"SONAR API SETUP\")\n",
    "\n",
    "    SONAR_API_KEY = \"<YOUR_SONAR_API_KEY>\"\n",
    "\n",
    "    # Test API connection\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {SONAR_API_KEY}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "\n",
    "    log_metric(\"API key configured\", \"✓\")\n",
    "    log_metric(\"Headers prepared\", headers)\n",
    "\n",
    "    return {\n",
    "        \"api_key\": SONAR_API_KEY,\n",
    "        \"headers\": headers,\n",
    "        \"base_url\": \"ADD URL FOR LLM\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ysqU89N3P2sp"
   },
   "source": [
    "# EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u325dGWskJP9"
   },
   "outputs": [],
   "source": [
    "# evlauate the inference techniques\n",
    "\n",
    "def evaluate_prompt_strategies(api_config, test_data, prompts, model=\"sonar\"):\n",
    "    log_section(f\"EVALUATING PROMPT STRATEGIES - MODEL: {model}\")\n",
    "\n",
    "    # Define prompt strategies to test\n",
    "    strategies = {\n",
    "        \"base\": {\n",
    "            \"name\": \"Base Prompt\",\n",
    "            \"description\": \"Simple system message without emotion or severity tailoring\",\n",
    "            \"system_message\": \"You are a helpful voice therapy assistant. Provide supportive, empathetic responses to users seeking help with voice-related concerns.\"\n",
    "        },\n",
    "        \"emotion_only\": {\n",
    "            \"name\": \"Emotion-Tailored\",\n",
    "            \"description\": \"System message tailored to detected emotion\",\n",
    "            \"get_system_message\": lambda example: prompts[\"system_prompts\"][f\"{example['emotion']}_medium\"][\"system_message\"]\n",
    "        },\n",
    "        \"severity_only\": {\n",
    "            \"name\": \"Severity-Tailored\",\n",
    "            \"description\": \"System message tailored to detected severity\",\n",
    "            \"get_system_message\": lambda example: prompts[\"system_prompts\"][f\"neutral_{example['severity']}\"][\"system_message\"]\n",
    "        },\n",
    "        \"emotion_severity\": {\n",
    "            \"name\": \"Fully Tailored\",\n",
    "            \"description\": \"System message tailored to both emotion and severity\",\n",
    "            \"get_system_message\": lambda example: prompts[\"system_prompts\"][f\"{example['emotion']}_{example['severity']}\"][\"system_message\"]\n",
    "        },\n",
    "        \"few_shot\": {\n",
    "            \"name\": \"Few-Shot Learning\",\n",
    "            \"description\": \"Includes a relevant example before the user query\",\n",
    "            \"get_system_message\": lambda example: prompts[\"system_prompts\"][f\"{example['emotion']}_{example['severity']}\"][\"system_message\"],\n",
    "            \"include_few_shot\": True\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Process a subset of examples to avoid excessive API calls\n",
    "    sample_size = min(25, len(test_data))\n",
    "    log_metric(\"Evaluating sample size\", sample_size)\n",
    "    test_sample = test_data[:sample_size]\n",
    "\n",
    "    # Results storage\n",
    "    all_results = {}\n",
    "\n",
    "    # Evaluate each strategy\n",
    "    for strategy_key, strategy in strategies.items():\n",
    "        log_step(f\"Testing strategy: {strategy['name']}\")\n",
    "\n",
    "        results = []\n",
    "\n",
    "        for i, example in enumerate(tqdm(test_sample)):\n",
    "            # Extract data\n",
    "            user_input = example[\"input\"]\n",
    "            expected_output = example[\"expected_output\"]\n",
    "            emotion = example[\"emotion\"]\n",
    "            severity = example[\"severity\"]\n",
    "\n",
    "            # Get system message based on strategy\n",
    "            if \"get_system_message\" in strategy:\n",
    "                system_message = strategy[\"get_system_message\"](example)\n",
    "            else:\n",
    "                system_message = strategy[\"system_message\"]\n",
    "\n",
    "            # Prepare messages\n",
    "            messages = [\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": system_message\n",
    "                }\n",
    "            ]\n",
    "\n",
    "            # Add few-shot example if strategy includes it\n",
    "            if strategy.get(\"include_few_shot\", False):\n",
    "                few_shot_key = f\"{emotion}_{severity}\"\n",
    "                if few_shot_key in prompts[\"few_shot_examples\"]:\n",
    "                    few_shot = prompts[\"few_shot_examples\"][few_shot_key]\n",
    "                    if few_shot[\"input\"] != \"No example available for this combination\":\n",
    "                        messages.append({\n",
    "                            \"role\": \"user\",\n",
    "                            \"content\": few_shot[\"input\"]\n",
    "                        })\n",
    "                        messages.append({\n",
    "                            \"role\": \"assistant\",\n",
    "                            \"content\": few_shot[\"output\"]\n",
    "                        })\n",
    "\n",
    "            # Add user input\n",
    "            messages.append({\n",
    "                \"role\": \"user\",\n",
    "                \"content\": user_input\n",
    "            })\n",
    "\n",
    "            # Prepare request\n",
    "            url = f\"{api_config['base_url']}/chat/completions\"\n",
    "            payload = {\n",
    "                \"model\": model,\n",
    "                \"messages\": messages\n",
    "            }\n",
    "\n",
    "            try:\n",
    "                # Make API call\n",
    "                response = requests.post(url, headers=api_config[\"headers\"], json=payload)\n",
    "\n",
    "                if response.status_code == 200:\n",
    "                    response_data = response.json()\n",
    "                    model_output = response_data[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "                    # Calculate simple metrics\n",
    "                    output_length = len(model_output)\n",
    "                    expected_length = len(expected_output)\n",
    "                    length_ratio = min(output_length, expected_length) / max(output_length, expected_length)\n",
    "\n",
    "                    # Store result\n",
    "                    result = {\n",
    "                        \"id\": i,\n",
    "                        \"input\": user_input,\n",
    "                        \"expected_output\": expected_output,\n",
    "                        \"model_output\": model_output,\n",
    "                        \"emotion\": emotion,\n",
    "                        \"severity\": severity,\n",
    "                        \"metrics\": {\n",
    "                            \"output_length\": output_length,\n",
    "                            \"expected_length\": expected_length,\n",
    "                            \"length_ratio\": length_ratio\n",
    "                        }\n",
    "                    }\n",
    "\n",
    "                    results.append(result)\n",
    "\n",
    "                    log_metric(f\"Example {i} processed\", \"✓\")\n",
    "                else:\n",
    "                    log_metric(f\"Example {i} failed\", f\"Status code: {response.status_code}\")\n",
    "\n",
    "                # Add a small delay to avoid rate limiting\n",
    "                time.sleep(0.5)\n",
    "\n",
    "            except Exception as e:\n",
    "                log_metric(f\"Example {i} error\", str(e))\n",
    "\n",
    "        # Save strategy results\n",
    "        all_results[strategy_key] = results\n",
    "\n",
    "        # Save to file\n",
    "        with open(f\"sonar_evaluation/{strategy_key}_results.json\", \"w\") as f:\n",
    "            json.dump(results, f, indent=2)\n",
    "\n",
    "        log_metric(f\"Strategy results saved\", f\"sonar_evaluation/{strategy_key}_results.json\")\n",
    "\n",
    "    # Analyze and compare strategies\n",
    "    analyze_strategy_results(all_results, strategies)\n",
    "\n",
    "    return all_results\n",
    "\n",
    "\n",
    "# analyze and compare the results of different prompt strategies.\n",
    "def analyze_strategy_results(all_results, strategies):\n",
    "    log_section(\"ANALYZING PROMPT STRATEGY RESULTS\")\n",
    "\n",
    "    strategy_metrics = {}\n",
    "\n",
    "    for strategy_key, results in all_results.items():\n",
    "        strategy_name = strategies[strategy_key][\"name\"]\n",
    "\n",
    "        # Skip if no results\n",
    "        if not results:\n",
    "            continue\n",
    "\n",
    "        # Calculate average metrics\n",
    "        avg_length_ratio = np.mean([r[\"metrics\"][\"length_ratio\"] for r in results])\n",
    "\n",
    "        # Calculate metrics by emotion\n",
    "        emotion_metrics = {}\n",
    "        for emotion in [\"anxiety\", \"sadness\", \"frustration\", \"hope\", \"neutral\", \"fear\", \"gratitude\"]:\n",
    "            emotion_results = [r for r in results if r[\"emotion\"] == emotion]\n",
    "            if emotion_results:\n",
    "                emotion_metrics[emotion] = {\n",
    "                    \"count\": len(emotion_results),\n",
    "                    \"avg_length_ratio\": np.mean([r[\"metrics\"][\"length_ratio\"] for r in emotion_results])\n",
    "                }\n",
    "\n",
    "        # Calculate metrics by severity\n",
    "        severity_metrics = {}\n",
    "        for severity in [\"low\", \"medium\", \"high\"]:\n",
    "            severity_results = [r for r in results if r[\"severity\"] == severity]\n",
    "            if severity_results:\n",
    "                severity_metrics[severity] = {\n",
    "                    \"count\": len(severity_results),\n",
    "                    \"avg_length_ratio\": np.mean([r[\"metrics\"][\"length_ratio\"] for r in severity_results])\n",
    "                }\n",
    "\n",
    "        # Store metrics\n",
    "        strategy_metrics[strategy_key] = {\n",
    "            \"name\": strategy_name,\n",
    "            \"sample_size\": len(results),\n",
    "            \"avg_length_ratio\": avg_length_ratio,\n",
    "            \"emotion_metrics\": emotion_metrics,\n",
    "            \"severity_metrics\": severity_metrics\n",
    "        }\n",
    "\n",
    "    # Save metrics to file\n",
    "    with open(\"sonar_evaluation/strategy_metrics.json\", \"w\") as f:\n",
    "        json.dump(strategy_metrics, f, indent=2)\n",
    "\n",
    "    log_metric(\"Strategy metrics saved\", \"sonar_evaluation/strategy_metrics.json\")\n",
    "\n",
    "    # Create comparison visualizations\n",
    "    log_step(\"Generating strategy comparison visualizations\")\n",
    "\n",
    "    # Overall comparison\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    strategies_list = list(strategy_metrics.keys())\n",
    "    avg_ratios = [strategy_metrics[s][\"avg_length_ratio\"] for s in strategies_list]\n",
    "    strategy_names = [strategy_metrics[s][\"name\"] for s in strategies_list]\n",
    "\n",
    "    sns.barplot(x=strategy_names, y=avg_ratios)\n",
    "    plt.title(\"Prompt Strategy Comparison - Average Length Ratio\")\n",
    "    plt.xlabel(\"Strategy\")\n",
    "    plt.ylabel(\"Avg Length Ratio (higher is better)\")\n",
    "    plt.ylim(0, 1)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"sonar_evaluation/strategy_comparison.png\")\n",
    "\n",
    "    # Emotion comparison for the best strategy\n",
    "    best_strategy = max(strategies_list, key=lambda s: strategy_metrics[s][\"avg_length_ratio\"])\n",
    "    best_strategy_name = strategy_metrics[best_strategy][\"name\"]\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    emotions = list(strategy_metrics[best_strategy][\"emotion_metrics\"].keys())\n",
    "    emotion_ratios = [strategy_metrics[best_strategy][\"emotion_metrics\"][e][\"avg_length_ratio\"] for e in emotions]\n",
    "\n",
    "    sns.barplot(x=emotions, y=emotion_ratios)\n",
    "    plt.title(f\"Emotion Performance - {best_strategy_name}\")\n",
    "    plt.xlabel(\"Emotion\")\n",
    "    plt.ylabel(\"Avg Length Ratio (higher is better)\")\n",
    "    plt.ylim(0, 1)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"sonar_evaluation/best_strategy_emotion_comparison.png\")\n",
    "\n",
    "    # Severity comparison for the best strategy\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    severities = list(strategy_metrics[best_strategy][\"severity_metrics\"].keys())\n",
    "    severity_ratios = [strategy_metrics[best_strategy][\"severity_metrics\"][s][\"avg_length_ratio\"] for s in severities]\n",
    "\n",
    "    sns.barplot(x=severities, y=severity_ratios)\n",
    "    plt.title(f\"Severity Performance - {best_strategy_name}\")\n",
    "    plt.xlabel(\"Severity\")\n",
    "    plt.ylabel(\"Avg Length Ratio (higher is better)\")\n",
    "    plt.ylim(0, 1)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"sonar_evaluation/best_strategy_severity_comparison.png\")\n",
    "\n",
    "    log_metric(\"Visualizations saved\", \"sonar_evaluation/strategy_comparison.png, sonar_evaluation/best_strategy_emotion_comparison.png, sonar_evaluation/best_strategy_severity_comparison.png\")\n",
    "\n",
    "    # Print summary of best strategy\n",
    "    log_step(\"Best Prompt Strategy Summary\")\n",
    "    log_metric(\"Best strategy\", best_strategy_name)\n",
    "    log_metric(\"Average length ratio\", f\"{strategy_metrics[best_strategy]['avg_length_ratio']:.4f}\")\n",
    "\n",
    "    # Return best strategy\n",
    "    return best_strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3e51jVDnP532"
   },
   "source": [
    "# DEPLOYMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "08QlRcOKkRKh"
   },
   "outputs": [],
   "source": [
    "def prepare_for_deployment(best_strategy):\n",
    "    \"\"\"\n",
    "    Prepare the optimized prompts for deployment.\n",
    "    \"\"\"\n",
    "    log_section(\"DEPLOYMENT PREPARATION\")\n",
    "\n",
    "    # Create deployment directory\n",
    "    os.makedirs(\"deployment_package\", exist_ok=True)\n",
    "\n",
    "    # Copy optimized prompts\n",
    "    import shutil\n",
    "    if os.path.exists(\"sonar_prompts/optimized_prompts.json\"):\n",
    "        shutil.copy(\"sonar_prompts/optimized_prompts.json\", \"deployment_package/\")\n",
    "\n",
    "    if os.path.exists(\"sonar_prompts/few_shot_examples.json\"):\n",
    "        shutil.copy(\"sonar_prompts/few_shot_examples.json\", \"deployment_package/\")\n",
    "\n",
    "    # Create a simple inference script\n",
    "\n",
    "    inference_script = \"\"\" # zdd more to this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "29plQIg9SW3q"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TFpdkAouSbAp"
   },
   "source": [
    "WORKFLOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E7DpDJsalxSM"
   },
   "outputs": [],
   "source": [
    "def run_sonar_prompt_engineering_workflow(train_clean, val_clean, test_clean):\n",
    "  try:\n",
    "        log_section(\"Starting Sonar Prompt Engineering Workflow\")\n",
    "\n",
    "\n",
    "        \"...\"\n",
    "\n",
    "    except Exception as e:\n",
    "        log_section(\"ERROR\")\n",
    "        log_step(f\"Workflow failed: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "        return {\n",
    "            \"status\": \"error\",\n",
    "            \"message\": str(e)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KyzAiP93l8yA"
   },
   "outputs": [],
   "source": [
    "# result = run_sonar_prompt_engineering_workflow(train_clean, val_clean, test_clean)\n",
    "#\n",
    "# print(f\"Workflow status: {result['status']}\")\n",
    "# if result['status'] == 'success':\n",
    "#     print(\"Outputs:\")\n",
    "#     for key, value in result['outputs'].items():\n",
    "#         print(f\"  {key}: {value}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
